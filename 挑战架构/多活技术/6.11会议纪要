

6 月份完成纸面上的研究成果
7 月底出具测试报告

1. 如果可以进行远程机房相关的中间件测试方案,如何去测试
    a. 先进行中间件测试
    b. 再进行相关集成测试
    原则上,  先容易后复杂,  先进行中间件分解测试,再进行所有组件依赖测试,最后所有的总集成多活测试

2. 人力系统的全局分析, Pass 能力平台 基础平台部 ,  通用能力平台 Pass  对我们人力系统的影响范围面
   倒推  能力平台支撑需求
   对于外部依赖条件的要求整理,需要外部支持什么样的功能--需要询问德勤公用能力调用支撑方式



发生故障时提供一个备用的位置来获取资源或者能提供可简易调整流量的装置，或两者都能提供

GSLB会替代最终的DNS的服务器从而实现自己的解析策略，返回给用户最合适的IP(列表)

一个普通的DNS请求：

① 用户提交域名
② 客户端解析域名
③ DNS服务器解析出IP
④ 客户端请求IP
⑤ 返回结束


加入了GSLB的请求：

① 提交域名
② 客户端解析域名
③ NS解析到GSLB-
④ GSLB解析并返回IP
⑤ 客户端请求IP
⑥ 返回结束

特点：

这个技术对原业务的侵入性最小，被商业ADC广泛实现，如A10，F5等。

但是可以得到的信息很有限，IP的定位只能靠Local DNS，因为得不到源IP.


a. 请求的域名均解析为GSLB机器的IP.
b. GSLB根据源IP等信息解析出新的IP并使用HTTP重定向技术将用户请求重定向到目标主机.


① 提交域名
② 客户端解析域名
③ DNS解析域名为GSLB
④ 客户端提交请求给GSLB服务器
⑤ GSLB解析出目标IP并发起HTTP转发
⑥ 客户读转发请求到目标IP
⑦ 返回结束


特点：
这个方案只适用于HTTP.
这个方案的实现可以是L7负载均衡工具如Nginx、HTTPD等

更改IP首部实现使用跳转.并利用IP tunneling技术实现只对请求负载均衡(响应直接返回).
a. 请求的域名均解析为GSLB机器的IP.
b. 负载均衡设备可以解析出目标地址,然后封装IP包发给目标地址.
c. 目标服务器收到请求包并处理,解析出被封装的IP包可以得到客户端地址,把响应直接返回.

① 提交域名
② 客户端解析域名
③ DNS解析域名为GSLB-
④ 客户端提交请求给GSLB服务器
⑤ GSLB发送请求到目标服务器
⑥ 目标服务器直接返回请求给客户端结束

这个方案能解决不能获得源IP和HTTP Only的问题，也不会成为性能瓶颈.但由于是IP层的LB，因此得到的信息很有限，也就是做分发的策略会很有限.*
实现方式主要就是LVS的VS/TUN模式.如果自己编写会更灵活，但难度较大.

客户端SDK+调度服务完成GSLB设备的功能。
a. 客户端使用原地址请求服务时，SDK会交付一个解析过的地址给客户端.(或对网络请求模块做Proxy)
b. SDK会通过一定的策略从调度服务中获取解析地址(一个或多个).
c. SDK和调度服务会某种形式保持联系(HTTP or TCP).
d. 出于性能考虑，SDK本身有Cache功能同时有时效限制(TTL)。

调用试请求过程:

① 客户端请求
② 调用SDK
③ SDK没有命中缓存
④ SDK请求调度服务
⑤ 调度服务返回新地址
⑥ 客户端用新地址发起请求
代理式请求过程:

① 客户端发起请求
② 网络请求Proxy拦截请求
③ Proxy没有命中缓存
④ Proxy请求调度服务
⑤ 调度服务返回新地址
⑥ Proxy请求新地址

特点

让客户端(SDK)具备了负载均衡知识,而因此让服务端可以获得任何想要知道的信息，从而可以做更全面的解析策略，但侵入性是最大的。
常见策略实现

地理区域。地理&IP表。
IP权重，为每个IP分配权重，权重决定流量比例。
往返时间RTT，分active RTT(请求时ping)和passive RTT(采集tcp的syn->act的时间)。
业务自定义条件,如根据语言,UserID等.




分布式唯一ID生成器

在应用程序中，经常需要全局唯一的ID作为数据库主键。如何生成全局唯一ID？

首先，需要确定全局唯一ID是整型还是字符串？如果是字符串，那么现有的UUID就完全满足需求，不需要额外的工作。缺点是字符串作为ID占用空间大，索引效率比整型低。

如果采用整型作为ID，那么首先排除掉32位int类型，因为范围太小，必须使用64位long型。

采用整型作为ID时，如何生成自增、全局唯一且不重复的ID？

方案一：利用数据库的自增ID，从1开始，基本可以做到连续递增。Oracle可以用SEQUENCE，MySQL可以用主键的AUTO_INCREMENT，虽然不能保证全局唯一，但每个表唯一，也基本满足需求。

数据库自增ID的缺点是数据在插入前，无法获得ID。数据在插入后，获取的ID虽然是唯一的，但一定要等到事务提交后，ID才算是有效的。有些双向引用的数据，不得不插入后再做一次更新，比较麻烦。

第二种方式是采用一个集中式ID生成器，它可以是Redis，也可以是ZooKeeper，也可以利用数据库的表记录最后分配的ID。

这种方式最大的缺点是复杂性太高，需要严重依赖第三方服务，而且代码配置繁琐。一般来说，越是复杂的方案，越不可靠，并且测试越痛苦。

第三种方式是类似Twitter的Snowflake算法，它给每台机器分配一个唯一标识，然后通过时间戳+标识+自增实现全局唯一ID。这种方式好处在于ID生成算法完全是一个无状态机，无网络调用，高效可靠。缺点是如果唯一标识有重复，会造成ID冲突。

Snowflake算法采用41bit毫秒时间戳，加上10bit机器ID，加上12bit序列号，理论上最多支持1024台机器每秒生成4096000个序列号，对于Twitter的规模来说够用了。

但是对于绝大部分普通应用程序来说，根本不需要每秒超过400万的ID，机器数量也达不到1024台，所以，我们可以改进一下，使用更短的ID生成方式：

53bitID由32bit秒级时间戳+16bit自增+5bit机器标识组成，累积32台机器，每秒可以生成6.5万个序列号，核心代码：
private static synchronized long nextId(long epochSecond) {
    if (epochSecond < lastEpoch) {
        // warning: clock is turn back:
        logger.warn("clock is back: " + epochSecond + " from previous:" + lastEpoch);
        epochSecond = lastEpoch;
    }
    if (lastEpoch != epochSecond) {
        lastEpoch = epochSecond;
        reset();
    }
    offset++;
    long next = offset & MAX_NEXT;
    if (next == 0) {
        logger.warn("maximum id reached in 1 second in epoch: " + epochSecond);
        return nextId(epochSecond + 1);
    }
    return generateId(epochSecond, next, SHARD_ID);
}

时间戳减去一个固定值，此方案最高可支持到2106年。

如果每秒6.5万个序列号不够怎么办？没关系，可以继续递增时间戳，向前“借”下一秒的6.5万个序列号。

同时还解决了时间回拨的问题。

机器标识采用简单的主机名方案，只要主机名符合host-1，host-2就可以自动提取机器标识，无需配置。

最后，为什么采用最多53位整型，而不是64位整型？这是因为考虑到大部分应用程序是Web应用，如果要和JavaScript打交道，由于JavaScript支持的最大整型就是53位，超过这个位数，JavaScript将丢失精度。因此，使用53位整数可以直接由JavaScript读取，而超过53位时，就必须转换成字符串才能保证JavaScript处理正确，这会给API接口带来额外的复杂度。这也是为什么新浪微博的API接口会同时返回id和idstr的原因。


import java.net.InetAddress;
import java.net.UnknownHostException;
import java.time.LocalDate;
import java.time.ZoneId;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


	/**
	 * 53 bits unique id:
	 *
	 * |--------|--------|--------|--------|--------|--------|--------|--------|
	 * |00000000|00011111|11111111|11111111|11111111|11111111|11111111|11111111|
	 * |--------|---xxxxx|xxxxxxxx|xxxxxxxx|xxxxxxxx|xxx-----|--------|--------|
	 * |--------|--------|--------|--------|--------|---xxxxx|xxxxxxxx|xxx-----|
	 * |--------|--------|--------|--------|--------|--------|--------|---xxxxx|
	 *
	 * Maximum ID = 11111_11111111_11111111_11111111_11111111_11111111_11111111
	 *
	 * Maximum TS = 11111_11111111_11111111_11111111_111
	 *
	 * Maximum NT = ----- -------- -------- -------- ---11111_11111111_111 = 65535
	 *
	 * Maximum SH = ----- -------- -------- -------- -------- -------- ---11111 = 31
	 *
	 * It can generate 64k unique id per IP and up to 2106-02-07T06:28:15Z.
	 */
	public final class IdUtil {


		private static final Logger logger = LoggerFactory.getLogger(IdUtil.class);


		private static final Pattern PATTERN_LONG_ID = Pattern.compile("^([0-9]{15})([0-9a-f]{32})([0-9a-f]{3})$");


		private static final Pattern PATTERN_HOSTNAME = Pattern.compile("^.*\\D+([0-9]+)$");


		private static final long OFFSET = LocalDate.of(2000, 1, 1).atStartOfDay(ZoneId.of("Z")).toEpochSecond();


		private static final long MAX_NEXT = 0b11111_11111111_111L;


		private static final long SHARD_ID = getServerIdAsLong();


		private static long offset = 0;


		private static long lastEpoch = 0;


		public static long nextId() {
			return nextId(System.currentTimeMillis() / 1000);
		}


		private static synchronized long nextId(long epochSecond) {
			if (epochSecond < lastEpoch) {
				// warning: clock is turn back:
				logger.warn("clock is back: " + epochSecond + " from previous:" + lastEpoch);
				epochSecond = lastEpoch;
			}
			if (lastEpoch != epochSecond) {
				lastEpoch = epochSecond;
				reset();
			}
			offset++;
			long next = offset & MAX_NEXT;
			if (next == 0) {
				logger.warn("maximum id reached in 1 second in epoch: " + epochSecond);
				return nextId(epochSecond + 1);
			}
			return generateId(epochSecond, next, SHARD_ID);
		}


		private static void reset() {
			offset = 0;
		}


		private static long generateId(long epochSecond, long next, long shardId) {
			return ((epochSecond - OFFSET) << 21) | (next << 5) | shardId;
		}


		private static long getServerIdAsLong() {
			try {
				String hostname = InetAddress.getLocalHost().getHostName();
				Matcher matcher = PATTERN_HOSTNAME.matcher(hostname);
				if (matcher.matches()) {
					long n = Long.parseLong(matcher.group(1));
					if (n >= 0 && n < 8) {
						logger.info("detect server id from host name {}: {}.", hostname, n);
						return n;
					}
				}
			} catch (UnknownHostException e) {
				logger.warn("unable to get host name. set server id = 0.");
			}
			return 0;
		}


		public static long stringIdToLongId(String stringId) {
			// a stringId id is composed as timestamp (15) + uuid (32) + serverId (000~fff).
			Matcher matcher = PATTERN_LONG_ID.matcher(stringId);
			if (matcher.matches()) {
				long epoch = Long.parseLong(matcher.group(1)) / 1000;
				String uuid = matcher.group(2);
				byte[] sha1 = HashUtil.sha1AsBytes(uuid);
				long next = ((sha1[0] << 24) | (sha1[1] << 16) | (sha1[2] << 8) | sha1[3]) & MAX_NEXT;
				long serverId = Long.parseLong(matcher.group(3), 16);
				return generateId(epoch, next, serverId);
			}
			throw new IllegalArgumentException("Invalid id: " + stringId);
		}
	}


import java.net.InetAddress;
import java.net.UnknownHostException;
import java.time.LocalDate;
import java.time.ZoneId;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


    /**
     * 53 bits unique id:
     *
     * |--------|--------|--------|--------|--------|--------|--------|--------|
     * |00000000|00011111|11111111|11111111|11111111|11111111|11111111|11111111|
     * |--------|---xxxxx|xxxxxxxx|xxxxxxxx|xxxxxxxx|xxx-----|--------|--------|
     * |--------|--------|--------|--------|--------|---xxxxx|xxxxxxxx|xxx-----|
     * |--------|--------|--------|--------|--------|--------|--------|---xxxxx|
     *
     * Maximum ID = 11111_11111111_11111111_11111111_11111111_11111111_11111111
     *
     * Maximum TS = 11111_11111111_11111111_11111111_111
     *
     * Maximum NT = ----- -------- -------- -------- ---11111_11111111_111 = 65535
     *
     * Maximum SH = ----- -------- -------- -------- -------- -------- ---11111 = 31
     *
     * It can generate 64k unique id per IP and up to 2106-02-07T06:28:15Z.
     */
    public final class IdUtil {


        private static final Logger logger = LoggerFactory.getLogger(IdUtil.class);


        private static final Pattern PATTERN_LONG_ID = Pattern.compile("^([0-9]{15})([0-9a-f]{32})([0-9a-f]{3})$");


        private static final Pattern PATTERN_HOSTNAME = Pattern.compile("^.*\\D+([0-9]+)$");


        private static final long OFFSET = LocalDate.of(2000, 1, 1).atStartOfDay(ZoneId.of("Z")).toEpochSecond();


        private static final long MAX_NEXT = 0b11111_11111111_111L;


        private static final long SHARD_ID = getServerIdAsLong();


        private static long offset = 0;


        private static long lastEpoch = 0;


        public static long nextId() {
            return nextId(System.currentTimeMillis() / 1000);
        }


        private static synchronized long nextId(long epochSecond) {
            if (epochSecond < lastEpoch) {
                // warning: clock is turn back:
                logger.warn("clock is back: " + epochSecond + " from previous:" + lastEpoch);
                epochSecond = lastEpoch;
            }
            if (lastEpoch != epochSecond) {
                lastEpoch = epochSecond;
                reset();
            }
            offset++;
            long next = offset & MAX_NEXT;
            if (next == 0) {
                logger.warn("maximum id reached in 1 second in epoch: " + epochSecond);
                return nextId(epochSecond + 1);
            }
            return generateId(epochSecond, next, SHARD_ID);
        }


        private static void reset() {
            offset = 0;
        }


        private static long generateId(long epochSecond, long next, long shardId) {
            return ((epochSecond - OFFSET) << 21) | (next << 5) | shardId;
        }


        private static long getServerIdAsLong() {
            try {
                String hostname = InetAddress.getLocalHost().getHostName();
                Matcher matcher = PATTERN_HOSTNAME.matcher(hostname);
                if (matcher.matches()) {
                    long n = Long.parseLong(matcher.group(1));
                    if (n >= 0 && n < 8) {
                        logger.info("detect server id from host name {}: {}.", hostname, n);
                        return n;
                    }
                }
            } catch (UnknownHostException e) {
                logger.warn("unable to get host name. set server id = 0.");
            }
            return 0;
        }


        public static long stringIdToLongId(String stringId) {
            // a stringId id is composed as timestamp (15) + uuid (32) + serverId (000~fff).
            Matcher matcher = PATTERN_LONG_ID.matcher(stringId);
            if (matcher.matches()) {
                long epoch = Long.parseLong(matcher.group(1)) / 1000;
                String uuid = matcher.group(2);
                byte[] sha1 = HashUtil.sha1AsBytes(uuid);
                long next = ((sha1[0] << 24) | (sha1[1] << 16) | (sha1[2] << 8) | sha1[3]) & MAX_NEXT;
                long serverId = Long.parseLong(matcher.group(3), 16);
                return generateId(epoch, next, serverId);
            }
            throw new IllegalArgumentException("Invalid id: " + stringId);
        }
    }





    Redis在携程内部得到了广泛的使用，根据客户端数据统计，整个携程全部Redis的读写请求在200W QPS/s，其中写请求约10W QPS/S，很多业务甚至会将Redis当成内存数据库使用。

    这样，就对Redis多数据中心提出了很大的需求，一是为了提升可用性，解决数据中心DR(DisasterRecovery)问题；二是提升访问性能，每个数据中心可以读取当前数据中心的数据，无需跨机房读数据。在这样的需求下，XPipe应运而生 。

    从实现的角度来说，XPipe主要需要解决三个方面的问题，一是数据复制，同时在复制的过程中保证数据的一致性；二是高可用，xpipe本身的高可用和Redis系统的高可用；三是如何在机房异常时，进行DR切换。

    下文将会从这三个方面对问题进行详细阐述。最后，将会对测试结果和系统在生产环境运行情况进行说明。

    为了方便描述，后面的行文中用DC代表数据中心(Data Center)。
    一、数据复制问题

    多数据中心首先要解决的是数据复制问题，即数据如何从一个DC传输到另外一个DC，通常有如下方案：

    客户端双写

    从客户端的角度来解决问题，单个客户端双写两个DC的服务器。初看没有什么问题。但是深入看下去，如果写入一个IDC成功，另外一个IDC失败，那数据可能会不一致，为了保证一致，可能需要先写入一个队列，然后再将队列的数据发送到两个IDC。 如果队列是本地队列，那当前服务器挂掉，数据可能会丢失；如果队列是远程队列，又给整体的方案带来了很大的复杂度。

    目前的应用一般都是集群部署，会有多个客户端同时操作。在多个客户端的前提下，又带来了新的问题。比如两个客户端ClientA和ClientB:

    ClientA:  set key value1
    ClientB:  set key value2

    由于两个客户端独立操作，到达服务器的顺序不可控，所以可能会导致两个DC的服务器对于同一个key，value不一致，如下：

    Server1: setkey value1; set key value2;
    Server2: setkey value2; set key value1;

    在Server1，最终值为value2，在Server2，最终值为value1。

    服务器代理

    proxy模式解决了多客户端写可能会导致数据不一致的问题。proxy类似于一个client，和单个client双写的问题类似，需要一个数据队列保数据一致性。

    为了提升系统的利用率，单个proxy往往需要代理多个Redis server，如果proxy出问题，会导致大面积的系统故障。这样，就对系统的性能和可用性提出了极大的挑战，带来实现的复杂度。

    此外，在特殊的情况下，仍然会可能带来数据的不一致，比如value和时间相关，或者是随机数，两个Redis服务器所在系统的不一致带来了数据的不一致。

    考虑到以上情况，为了解决复制问题，我们决定采用伪slave的方案，即实现Redis协议，伪装成为Redis slave，让Redis master推送数据至伪slave。这个伪slave，我们把它称为keeper，如下图所示：

    有了keeper之后，多数据中心之间的数据传输，可以通过keeper进行。keeper将Redis日志数据缓存到磁盘，这样，可以缓存大量的日志数据(Redis将数据缓存到内存ring buffer，容量有限)，当数据中心之间的网络出现较长时间异常时仍然可以续传日志数据。

    Redis协议不可更改，而keeper之间的数据传输协议却可以自定义。这样就可以进行压缩，以提升系统性能，节约传输成本；多个机房之间的数据传输往往需要通过公网进行，这样数据的安全性变得极为重要，keeper之间的数据传输也可以加密，提升安全性。
    二、高可用

    任何系统都可能会挂掉，如果keeper挂掉，多数据中心之间的数据传输可能会中断，为了解决这个问题，需要保证keeper的高可用。我们的方案中，keeper有主备两个节点，备节点实时从主节点复制数据，当主节点挂掉后，备节点会被提升为主节点，代替主节点进行服务。

    提升的操作需要通过第三方节点进行，我们把它称之为MetaServer，主要负责keeper状态的转化以及机房内部元信息的存储。同时MetaServer也要做到高可用：每个MetaServer负责特定的Redis集群，当有MetaServer节点挂掉时，其负责的Redis集群将由其它节点接替；如果整个集群中有新的节点接入，则会自动进行一次负载均衡，将部分集群移交到此新节点。

    Redis也可能会挂，Redis本身提供哨兵(Sentinel)机制保证集群的高可用。但是在Redis4.0版本之前，提升新的master后，其它节点连到此节点后都会进行全量同步，全量同步时，slave会处于不可用状态；master将会导出rdb，降低master的可用性；同时由于集群中有大量数据(RDB)传输，将会导致整体系统的不稳定。

    截止当前文章书写之时，4.0仍然没有发布release版本，而且携程内部使用的Redis版本为2.8.19，如果升到4.0，版本跨度太大，基于此，我们在Redis3.0.7的版本基础上进行优化，实现了psync2.0协议，实现了增量同步。下面是Redis作者对协议的介绍：https://gist.github.com/antirez/ae068f95c0d084891305。
    三、DR切换

    DR切换分为两种可能，一种是机房真的挂了或者出异常，需要进行切换，另外一种是机房仍然健康，但是由于演练、业务要求等原因仍然需要切换到另外的机房。XPipe处理机房切换的流程如下：

    检查是否可以进行DR切换
    类似于2PC协议，首先进行prepare，保证流程能顺利进行。
    原主机房master禁止写入
    此步骤，保证在迁移的过程中，只有一个master，解决在迁移过程中可能存在的数据丢失情况。
    提升新主机房master
    其它机房向新主机房同步

    当然了，即使做了检查，也无法绝对保证整个迁移过程肯定能够成功，为此，我们提供回滚和重试功能。回滚功能可以回滚到初始的状态，重试功能可以在DBA人工介入的前提下，修复异常条件，继续进行切换。

    根据以上分析，XPipe系统的整体架构如下所示：

    Console用来管理多机房的元信息数据，同时提供用户界面，供用户进行配置和DR切换等操作。Keeper负责缓存Redis操作日志，并对跨机房传输进行压缩、加密等处理。Meta Server管理单机房内的所有keeper状态，并对异常状态进行纠正。
    四、测试数据

    我们关注的重点在于增加keeper后，平均延时的增加。测试方式如下图所示。从client发送数据至master，并且slave通过keyspacenotification的方式通知到client，整个测试延时时间为t1+t2+t3。

    首先我们测试Redis master直接复制到slave的延时，为0.2ms。然后在master和slave之间增加一层keeper，整体延时增加0.1ms，到0.3ms。相较于多个DC之间几毫秒，几十毫秒的延时，增加一层keeper带来的延时是完全没问题的。

    在携程生产环境进行了测试，生产环境两个机房之间的ping TTL约为0.61ms，经过跨数据中心的两层keeper后，测试得到的平均延时约为0.8ms，延时99.9线为2ms。

    综上所述：XPipe主要解决Redis多数据中心数据同步以及DR切换问题，同时，由于XPipe增强后的Redis版本优化了psync协议，会极大的提升Redis集群的稳定性。

    同时，整个系统已经开源，欢迎大家一起参与优化整个系统：

    XPipe: https://github.com/ctripcorp/x-pipe

    XRedis(在Redis3.0.7版本上进行增强的版本)：
    https://github.com/ctripcorp/redis
